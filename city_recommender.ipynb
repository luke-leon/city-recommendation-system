{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxine original code for unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJkrCiAaxEyD",
    "outputId": "bc72ad81-7a3e-4b74-c139-f45d9dcf53db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lift Analysis for 'crime':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0        austin                  12            8.815534  1.361233\n",
      "1        dallas                   8            5.024274  1.592270\n",
      "2       houston                   6            5.467126  1.097469\n",
      "3       chicago                   0            7.343986  0.000000\n",
      "4      sandiego                   1            5.670510  0.176351\n",
      "5       phoenix                   0            9.200210  0.000000\n",
      "6  philadelphia                   7            2.399398  2.917398\n",
      "7           nyc                   4            0.837029  4.778806\n",
      "8    LosAngeles                   8            1.241932  6.441574\n",
      "\n",
      "Lift Analysis for 'cost':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0        austin                  41           43.311103  0.946639\n",
      "1        dallas                  26           24.684476  1.053294\n",
      "2       houston                  53           26.860227  1.973178\n",
      "3       chicago                  16           36.081321  0.443443\n",
      "4      sandiego                  41           27.859463  1.471672\n",
      "5       phoenix                  40           45.201031  0.884936\n",
      "6  philadelphia                   5           11.788348  0.424148\n",
      "7           nyc                   1            4.112361  0.243169\n",
      "8    LosAngeles                   3            6.101668  0.491669\n",
      "\n",
      "Lift Analysis for 'housing':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0        austin                  35           18.397637  1.902418\n",
      "1        dallas                   5           10.485441  0.476852\n",
      "2       houston                   3           11.409654  0.262935\n",
      "3       chicago                  12           15.326579  0.782954\n",
      "4      sandiego                  24           11.834108  2.028036\n",
      "5       phoenix                   6           19.200438  0.312493\n",
      "6  philadelphia                   6            5.007440  1.198217\n",
      "7           nyc                   2            1.746844  1.144922\n",
      "8    LosAngeles                   3            2.591859  1.157470\n",
      "\n",
      "Lift Analysis for 'jobs':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0        austin                  12           17.056142  0.703559\n",
      "1        dallas                   5            9.720878  0.514357\n",
      "2       houston                  16           10.577700  1.512616\n",
      "3       chicago                  16           14.209016  1.126046\n",
      "4      sandiego                  16           10.971205  1.458363\n",
      "5       phoenix                  10           17.800406  0.561785\n",
      "6  philadelphia                   6            4.642314  1.292459\n",
      "7           nyc                   3            1.619470  1.852458\n",
      "8    LosAngeles                   5            2.402869  2.080846\n",
      "\n",
      "Lift Analysis for 'transportation':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0        austin                   9           53.276490  0.168930\n",
      "1        dallas                   7           30.364090  0.230535\n",
      "2       houston                   6           33.040457  0.181596\n",
      "3       chicago                 232           44.383218  5.227201\n",
      "4      sandiego                   8           34.269605  0.233443\n",
      "5       phoenix                   7           55.601268  0.125896\n",
      "6  philadelphia                   3           14.500712  0.206886\n",
      "7           nyc                   5            5.058568  0.988422\n",
      "8    LosAngeles                   1            7.505592  0.133234\n",
      "\n",
      "Lift Analysis for 'weather':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0        austin                  23           24.338540  0.945003\n",
      "1        dallas                  15           13.871365  1.081364\n",
      "2       houston                  20           15.094022  1.325028\n",
      "3       chicago                  14           20.275787  0.690479\n",
      "4      sandiego                  19           15.655539  1.213628\n",
      "5       phoenix                  28           25.400579  1.102337\n",
      "6  philadelphia                   3            6.624426  0.452869\n",
      "7           nyc                   3            2.310929  1.298179\n",
      "8    LosAngeles                   2            3.428813  0.583292\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the total word counts CSV file\n",
    "total_word_counts = pd.read_csv('total_word_counts_by_city.csv')\n",
    "\n",
    "# List of important attributes (you can expand this list)\n",
    "attributes = ['crime', 'cost', 'housing', 'jobs', 'transportation', 'weather']\n",
    "\n",
    "# Initialize an empty dictionary to store lift data for all attributes\n",
    "lift_data = {}\n",
    "\n",
    "# Iterate over each attribute to calculate lift\n",
    "for attribute in attributes:\n",
    "    if attribute in total_word_counts['Unnamed: 0'].values:\n",
    "        # Get total count of the attribute across all cities\n",
    "        total_frequency_attribute = total_word_counts[total_word_counts['Unnamed: 0'] == attribute].drop(columns='Unnamed: 0').sum().sum()\n",
    "\n",
    "        # Calculate the total word count for each city (sum across all words)\n",
    "        total_words_per_city = total_word_counts.drop(columns='Unnamed: 0').sum()\n",
    "\n",
    "        # Get the observed frequency of the attribute in each city\n",
    "        observed_frequency_per_city = total_word_counts[total_word_counts['Unnamed: 0'] == attribute].drop(columns='Unnamed: 0').iloc[0]\n",
    "\n",
    "        # Calculate the expected frequency of the attribute in each city\n",
    "        expected_frequency_per_city = (total_frequency_attribute / total_words_per_city.sum()) * total_words_per_city\n",
    "\n",
    "        # Calculate lift for each city\n",
    "        lift_per_city = observed_frequency_per_city / expected_frequency_per_city\n",
    "\n",
    "        # Store the results for this attribute\n",
    "        lift_data[attribute] = pd.DataFrame({\n",
    "            'City': lift_per_city.index,\n",
    "            'Observed Frequency': observed_frequency_per_city.values,\n",
    "            'Expected Frequency': expected_frequency_per_city.values,\n",
    "            'Lift': lift_per_city.values\n",
    "        })\n",
    "    else:\n",
    "        print(f\"The word '{attribute}' was not found in the dataset.\")\n",
    "\n",
    "# Function to display lift data for each attribute\n",
    "def display_lift_data(lift_data):\n",
    "    for attribute, df in lift_data.items():\n",
    "        print(f\"\\nLift Analysis for '{attribute}':\")\n",
    "        print(df)\n",
    "\n",
    "# Call the function to display the lift data for each attribute\n",
    "display_lift_data(lift_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaked unweighted code so output is a lift matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lift Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>austin</th>\n",
       "      <th>dallas</th>\n",
       "      <th>houston</th>\n",
       "      <th>chicago</th>\n",
       "      <th>sandiego</th>\n",
       "      <th>phoenix</th>\n",
       "      <th>philadelphia</th>\n",
       "      <th>nyc</th>\n",
       "      <th>LosAngeles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>1.361233</td>\n",
       "      <td>1.592270</td>\n",
       "      <td>1.097469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.917398</td>\n",
       "      <td>4.778806</td>\n",
       "      <td>6.441574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost</th>\n",
       "      <td>0.946639</td>\n",
       "      <td>1.053294</td>\n",
       "      <td>1.973178</td>\n",
       "      <td>0.443443</td>\n",
       "      <td>1.471672</td>\n",
       "      <td>0.884936</td>\n",
       "      <td>0.424148</td>\n",
       "      <td>0.243169</td>\n",
       "      <td>0.491669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>1.902418</td>\n",
       "      <td>0.476852</td>\n",
       "      <td>0.262935</td>\n",
       "      <td>0.782954</td>\n",
       "      <td>2.028036</td>\n",
       "      <td>0.312493</td>\n",
       "      <td>1.198217</td>\n",
       "      <td>1.144922</td>\n",
       "      <td>1.157470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jobs</th>\n",
       "      <td>0.703559</td>\n",
       "      <td>0.514357</td>\n",
       "      <td>1.512616</td>\n",
       "      <td>1.126046</td>\n",
       "      <td>1.458363</td>\n",
       "      <td>0.561785</td>\n",
       "      <td>1.292459</td>\n",
       "      <td>1.852458</td>\n",
       "      <td>2.080846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transportation</th>\n",
       "      <td>0.168930</td>\n",
       "      <td>0.230535</td>\n",
       "      <td>0.181596</td>\n",
       "      <td>5.227201</td>\n",
       "      <td>0.233443</td>\n",
       "      <td>0.125896</td>\n",
       "      <td>0.206886</td>\n",
       "      <td>0.988422</td>\n",
       "      <td>0.133234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>0.945003</td>\n",
       "      <td>1.081364</td>\n",
       "      <td>1.325028</td>\n",
       "      <td>0.690479</td>\n",
       "      <td>1.213628</td>\n",
       "      <td>1.102337</td>\n",
       "      <td>0.452869</td>\n",
       "      <td>1.298179</td>\n",
       "      <td>0.583292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  austin    dallas   houston   chicago  sandiego   phoenix  \\\n",
       "crime           1.361233  1.592270  1.097469  0.000000  0.176351  0.000000   \n",
       "cost            0.946639  1.053294  1.973178  0.443443  1.471672  0.884936   \n",
       "housing         1.902418  0.476852  0.262935  0.782954  2.028036  0.312493   \n",
       "jobs            0.703559  0.514357  1.512616  1.126046  1.458363  0.561785   \n",
       "transportation  0.168930  0.230535  0.181596  5.227201  0.233443  0.125896   \n",
       "weather         0.945003  1.081364  1.325028  0.690479  1.213628  1.102337   \n",
       "\n",
       "                philadelphia       nyc  LosAngeles  \n",
       "crime               2.917398  4.778806    6.441574  \n",
       "cost                0.424148  0.243169    0.491669  \n",
       "housing             1.198217  1.144922    1.157470  \n",
       "jobs                1.292459  1.852458    2.080846  \n",
       "transportation      0.206886  0.988422    0.133234  \n",
       "weather             0.452869  1.298179    0.583292  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the total word counts CSV file\n",
    "total_word_counts = pd.read_csv('total_word_counts_by_city.csv')\n",
    "\n",
    "# List of important attributes\n",
    "attributes = ['crime', 'cost', 'housing', 'jobs', 'transportation', 'weather']\n",
    "\n",
    "# Initialize an empty DataFrame to store the lift matrix\n",
    "lift_matrix = pd.DataFrame()\n",
    "\n",
    "# Iterate over each attribute to calculate lift\n",
    "for attribute in attributes:\n",
    "    if attribute in total_word_counts['Unnamed: 0'].values:\n",
    "        # Get total count of the attribute across all cities\n",
    "        total_frequency_attribute = total_word_counts[total_word_counts['Unnamed: 0'] == attribute].drop(columns='Unnamed: 0').sum().sum()\n",
    "\n",
    "        # Calculate the total word count for each city (sum across all words)\n",
    "        total_words_per_city = total_word_counts.drop(columns='Unnamed: 0').sum()\n",
    "\n",
    "        # Get the observed frequency of the attribute in each city\n",
    "        observed_frequency_per_city = total_word_counts[total_word_counts['Unnamed: 0'] == attribute].drop(columns='Unnamed: 0').iloc[0]\n",
    "\n",
    "        # Calculate the expected frequency of the attribute in each city\n",
    "        expected_frequency_per_city = (total_frequency_attribute / total_words_per_city.sum()) * total_words_per_city\n",
    "\n",
    "        # Calculate lift for each city\n",
    "        lift_per_city = observed_frequency_per_city / expected_frequency_per_city\n",
    "\n",
    "        # Add the lift data for this attribute to the lift matrix\n",
    "        lift_matrix[attribute] = lift_per_city\n",
    "    else:\n",
    "        print(f\"The word '{attribute}' was not found in the dataset.\")\n",
    "\n",
    "# uncomment this line to Transpose the lift matrix to have cities as rows and attributes as columns\n",
    "lift_matrix = lift_matrix.transpose()\n",
    "\n",
    "# Display the final lift matrix\n",
    "print(\"Lift Matrix:\")\n",
    "lift_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxine original code for weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnRmXwSgxInr",
    "outputId": "073c5ee8-8c0b-498d-9ea2-7d80d184c597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lift Analysis for 'crime':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0    LosAngeles                 478          114.306029  4.181757\n",
      "1        austin                1051          847.075841  1.240739\n",
      "2       chicago                   0          661.495162  0.000000\n",
      "3        dallas                 397          288.217784  1.377431\n",
      "4       houston                 713          294.082591  2.424489\n",
      "5           nyc                 301          125.972759  2.389405\n",
      "6  philadelphia                  49          177.675551  0.275784\n",
      "7       phoenix                   0          544.305049  0.000000\n",
      "8      sandiego                 348          283.869234  1.225917\n",
      "\n",
      "Lift Analysis for 'cost':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0    LosAngeles                 175          375.733544  0.465756\n",
      "1        austin                1453         2784.409621  0.521834\n",
      "2       chicago                1160         2174.390301  0.533483\n",
      "3        dallas                1645          947.396127  1.736338\n",
      "4       houston                2605          966.674239  2.694806\n",
      "5           nyc                  75          414.083065  0.181123\n",
      "6  philadelphia                 313          584.034498  0.535927\n",
      "7       phoenix                1311         1789.176530  0.732739\n",
      "8      sandiego                2232          933.102076  2.392021\n",
      "\n",
      "Lift Analysis for 'housing':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0    LosAngeles                 117          134.824253  0.867796\n",
      "1        austin                1018          999.128113  1.018888\n",
      "2       chicago                 185          780.235229  0.237108\n",
      "3        dallas                 746          339.953611  2.194417\n",
      "4       houston                 204          346.871165  0.588115\n",
      "5           nyc                 168          148.585190  1.130665\n",
      "6  philadelphia                 189          209.568765  0.901852\n",
      "7       phoenix                 211          642.009192  0.328656\n",
      "8      sandiego                1098          334.824485  3.279330\n",
      "\n",
      "Lift Analysis for 'jobs':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0    LosAngeles                 681          284.651814  2.392396\n",
      "1        austin                3453         2109.439689  1.636928\n",
      "2       chicago                 542         1647.295414  0.329024\n",
      "3        dallas                   0          717.737425  0.000000\n",
      "4       houston                 980          732.342322  1.338172\n",
      "5           nyc                 194          313.705011  0.618415\n",
      "6  philadelphia                 212          442.458444  0.479141\n",
      "7       phoenix                2097         1355.461479  1.547075\n",
      "8      sandiego                 151          706.908401  0.213606\n",
      "\n",
      "Lift Analysis for 'transportation':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0    LosAngeles                  17          680.115736  0.024996\n",
      "1        austin                 143         5040.063180  0.028373\n",
      "2       chicago               16076         3935.866480  4.084488\n",
      "3        dallas                 541         1714.882861  0.315473\n",
      "4       houston                  51         1749.778194  0.029147\n",
      "5           nyc                  19          749.532251  0.025349\n",
      "6  philadelphia                 935         1057.161542  0.884444\n",
      "7       phoenix                1869         3238.590574  0.577103\n",
      "8      sandiego                 204         1689.009182  0.120781\n",
      "\n",
      "Lift Analysis for 'weather':\n",
      "           City  Observed Frequency  Expected Frequency      Lift\n",
      "0    LosAngeles                   7          708.957713  0.009874\n",
      "1        austin                7303         5253.799427  1.390042\n",
      "2       chicago                1098         4102.776557  0.267624\n",
      "3        dallas                1028         1787.606677  0.575071\n",
      "4       houston                 123         1823.981833  0.067435\n",
      "5           nyc                 147          781.318005  0.188144\n",
      "6  philadelphia                 172         1101.993071  0.156081\n",
      "7       phoenix               10374         3375.930955  3.072930\n",
      "8      sandiego                 445         1760.635762  0.252750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the weighted word counts CSV file\n",
    "weighted_word_counts = pd.read_csv('weighted_word_counts_by_city.csv')\n",
    "\n",
    "# List of important attributes (you can expand this list)\n",
    "attributes = ['crime', 'cost', 'housing', 'jobs', 'transportation', 'weather']\n",
    "\n",
    "# Initialize an empty dictionary to store lift data for all attributes\n",
    "lift_data = {}\n",
    "\n",
    "# Iterate over each attribute to calculate lift\n",
    "for attribute in attributes:\n",
    "    if attribute in weighted_word_counts['Word'].values:\n",
    "        # Get total weighted count of the attribute across all cities\n",
    "        weighted_frequency_attribute = weighted_word_counts[weighted_word_counts['Word'] == attribute].drop(columns='Word').sum().sum()\n",
    "\n",
    "        # Calculate the total weighted word count for each city (sum across all words)\n",
    "        total_weighted_words_per_city = weighted_word_counts.drop(columns='Word').sum()\n",
    "\n",
    "        # Get the observed weighted frequency of the attribute in each city\n",
    "        observed_weighted_frequency_per_city = weighted_word_counts[weighted_word_counts['Word'] == attribute].drop(columns='Word').iloc[0]\n",
    "\n",
    "        # Calculate the expected weighted frequency of the attribute in each city\n",
    "        expected_weighted_frequency_per_city = (weighted_frequency_attribute / total_weighted_words_per_city.sum()) * total_weighted_words_per_city\n",
    "\n",
    "        # Calculate lift for each city\n",
    "        lift_per_city_weighted = observed_weighted_frequency_per_city / expected_weighted_frequency_per_city\n",
    "\n",
    "        # Store the results for this attribute\n",
    "        lift_data[attribute] = pd.DataFrame({\n",
    "            'City': lift_per_city_weighted.index,\n",
    "            'Observed Frequency': observed_weighted_frequency_per_city.values,\n",
    "            'Expected Frequency': expected_weighted_frequency_per_city.values,\n",
    "            'Lift': lift_per_city_weighted.values\n",
    "        })\n",
    "    else:\n",
    "        print(f\"The word '{attribute}' was not found in the dataset.\")\n",
    "\n",
    "# Function to display lift data for each attribute\n",
    "def display_lift_data(lift_data):\n",
    "    for attribute, df in lift_data.items():\n",
    "        print(f\"\\nLift Analysis for '{attribute}':\")\n",
    "        print(df)\n",
    "\n",
    "# Call the function to display the lift data for each attribute\n",
    "display_lift_data(lift_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaked weighted code so output is a lift matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Lift Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LosAngeles</th>\n",
       "      <th>austin</th>\n",
       "      <th>chicago</th>\n",
       "      <th>dallas</th>\n",
       "      <th>houston</th>\n",
       "      <th>nyc</th>\n",
       "      <th>philadelphia</th>\n",
       "      <th>phoenix</th>\n",
       "      <th>sandiego</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>4.181757</td>\n",
       "      <td>1.240739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.377431</td>\n",
       "      <td>2.424489</td>\n",
       "      <td>2.389405</td>\n",
       "      <td>0.275784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.225917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost</th>\n",
       "      <td>0.465756</td>\n",
       "      <td>0.521834</td>\n",
       "      <td>0.533483</td>\n",
       "      <td>1.736338</td>\n",
       "      <td>2.694806</td>\n",
       "      <td>0.181123</td>\n",
       "      <td>0.535927</td>\n",
       "      <td>0.732739</td>\n",
       "      <td>2.392021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>0.867796</td>\n",
       "      <td>1.018888</td>\n",
       "      <td>0.237108</td>\n",
       "      <td>2.194417</td>\n",
       "      <td>0.588115</td>\n",
       "      <td>1.130665</td>\n",
       "      <td>0.901852</td>\n",
       "      <td>0.328656</td>\n",
       "      <td>3.279330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jobs</th>\n",
       "      <td>2.392396</td>\n",
       "      <td>1.636928</td>\n",
       "      <td>0.329024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.338172</td>\n",
       "      <td>0.618415</td>\n",
       "      <td>0.479141</td>\n",
       "      <td>1.547075</td>\n",
       "      <td>0.213606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transportation</th>\n",
       "      <td>0.024996</td>\n",
       "      <td>0.028373</td>\n",
       "      <td>4.084488</td>\n",
       "      <td>0.315473</td>\n",
       "      <td>0.029147</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.577103</td>\n",
       "      <td>0.120781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>0.009874</td>\n",
       "      <td>1.390042</td>\n",
       "      <td>0.267624</td>\n",
       "      <td>0.575071</td>\n",
       "      <td>0.067435</td>\n",
       "      <td>0.188144</td>\n",
       "      <td>0.156081</td>\n",
       "      <td>3.072930</td>\n",
       "      <td>0.252750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                LosAngeles    austin   chicago    dallas   houston       nyc  \\\n",
       "crime             4.181757  1.240739  0.000000  1.377431  2.424489  2.389405   \n",
       "cost              0.465756  0.521834  0.533483  1.736338  2.694806  0.181123   \n",
       "housing           0.867796  1.018888  0.237108  2.194417  0.588115  1.130665   \n",
       "jobs              2.392396  1.636928  0.329024  0.000000  1.338172  0.618415   \n",
       "transportation    0.024996  0.028373  4.084488  0.315473  0.029147  0.025349   \n",
       "weather           0.009874  1.390042  0.267624  0.575071  0.067435  0.188144   \n",
       "\n",
       "                philadelphia   phoenix  sandiego  \n",
       "crime               0.275784  0.000000  1.225917  \n",
       "cost                0.535927  0.732739  2.392021  \n",
       "housing             0.901852  0.328656  3.279330  \n",
       "jobs                0.479141  1.547075  0.213606  \n",
       "transportation      0.884444  0.577103  0.120781  \n",
       "weather             0.156081  3.072930  0.252750  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the weighted word counts CSV file\n",
    "weighted_word_counts = pd.read_csv('weighted_word_counts_by_city.csv')\n",
    "\n",
    "# List of important attributes\n",
    "attributes = ['crime', 'cost', 'housing', 'jobs', 'transportation', 'weather']\n",
    "\n",
    "# Initialize an empty DataFrame to store the lift matrix\n",
    "lift_matrix_weighted = pd.DataFrame()\n",
    "\n",
    "# Iterate over each attribute to calculate lift\n",
    "for attribute in attributes:\n",
    "    if attribute in weighted_word_counts['Word'].values:\n",
    "        # Get total weighted count of the attribute across all cities\n",
    "        weighted_frequency_attribute = weighted_word_counts[weighted_word_counts['Word'] == attribute].drop(columns='Word').sum().sum()\n",
    "\n",
    "        # Calculate the total weighted word count for each city (sum across all words)\n",
    "        total_weighted_words_per_city = weighted_word_counts.drop(columns='Word').sum()\n",
    "\n",
    "        # Get the observed weighted frequency of the attribute in each city\n",
    "        observed_weighted_frequency_per_city = weighted_word_counts[weighted_word_counts['Word'] == attribute].drop(columns='Word').iloc[0]\n",
    "\n",
    "        # Calculate the expected weighted frequency of the attribute in each city\n",
    "        expected_weighted_frequency_per_city = (weighted_frequency_attribute / total_weighted_words_per_city.sum()) * total_weighted_words_per_city\n",
    "\n",
    "        # Calculate lift for each city\n",
    "        lift_per_city_weighted = observed_weighted_frequency_per_city / expected_weighted_frequency_per_city\n",
    "\n",
    "        # Add the lift data for this attribute to the lift matrix\n",
    "        lift_matrix_weighted[attribute] = lift_per_city_weighted\n",
    "    else:\n",
    "        print(f\"The word '{attribute}' was not found in the dataset.\")\n",
    "\n",
    "# uncomment this line to Transpose the lift matrix to have cities as rows and attributes as columns\n",
    "lift_matrix_weighted = lift_matrix_weighted.transpose()\n",
    "\n",
    "# Display the final lift matrix\n",
    "print(\"Weighted Lift Matrix:\")\n",
    "lift_matrix_weighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender system Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\krgod\\anaconda3\\Lib\\site-packages\\numpy-2.1.1.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install sentence-transformers --quiet\n",
    "!pip install vaderSentiment --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Step 1: Load and Prepare the Data\n",
    "reddit_posts_file = 'reddit_posts_cities.csv'\n",
    "reddit_df = pd.read_csv(reddit_posts_file)\n",
    "\n",
    "# List the columns that contain messages\n",
    "message_columns = ['Title', 'Body', 'Comment 1', 'Comment 2', 'Comment 3', 'Comment 4', 'Comment 5']\n",
    "\n",
    "# Combine the messages into one column per row\n",
    "reddit_df['Combined_Messages'] = reddit_df[message_columns].fillna('').apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "# Ensure 'Subreddit' is treated as a string\n",
    "reddit_df['Subreddit'] = reddit_df['Subreddit'].astype(str)\n",
    "\n",
    "# Step 2: Define User Preferences (Attributes)\n",
    "attributes_of_interest = ['safe', 'rent', 'clean']\n",
    "\n",
    "# Step 3: Load Sentence Transformer Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model suitable for semantic similarity\n",
    "\n",
    "# Step 4: Create Embeddings for Attributes\n",
    "attribute_embeddings = model.encode(attributes_of_interest, convert_to_numpy=True)\n",
    "mean_attribute_embedding = np.mean(attribute_embeddings, axis=0)\n",
    "\n",
    "# Step 5: Encode Individual Messages and Assign to DataFrame\n",
    "embeddings = model.encode(\n",
    "    reddit_df['Combined_Messages'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=32\n",
    ")\n",
    "reddit_df['Message_Embedding'] = list(embeddings)\n",
    "\n",
    "# Step 6: Compute Mean Embeddings for Each City\n",
    "city_embeddings = reddit_df.groupby('Subreddit')['Message_Embedding'].apply(\n",
    "    lambda embeddings: np.mean(np.vstack(embeddings), axis=0)\n",
    ").reset_index()\n",
    "city_embeddings.rename(columns={'Subreddit': 'City'}, inplace=True)\n",
    "\n",
    "# Step 7: Calculate Similarity Scores\n",
    "city_embeddings['Similarity'] = city_embeddings['Message_Embedding'].apply(\n",
    "    lambda emb: util.cos_sim(emb, mean_attribute_embedding).item()\n",
    ")\n",
    "\n",
    "# Step 8: Sentiment Analysis on Messages Mentioning Attributes Using VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to check if a message mentions any of the attributes\n",
    "def mentions_attributes(text, attributes):\n",
    "    tokens = text.lower().split()\n",
    "    return any(attr.lower() in tokens for attr in attributes)\n",
    "\n",
    "# Identify messages that mention the attributes\n",
    "reddit_df['Mentions_Attributes'] = reddit_df['Combined_Messages'].apply(\n",
    "    lambda text: mentions_attributes(text, attributes_of_interest)\n",
    ")\n",
    "\n",
    "# Filter messages that mention the attributes\n",
    "attribute_mentions_df = reddit_df[reddit_df['Mentions_Attributes']].copy()\n",
    "\n",
    "# Apply sentiment analysis to messages that mention attributes\n",
    "attribute_mentions_df['Sentiment_Score'] = attribute_mentions_df['Combined_Messages'].apply(\n",
    "    lambda text: analyzer.polarity_scores(text)['compound']\n",
    ")\n",
    "\n",
    "# Step 9: Compute Average Sentiment Scores for Each City\n",
    "city_sentiment = attribute_mentions_df.groupby('Subreddit')['Sentiment_Score'].mean().reset_index()\n",
    "city_sentiment.columns = ['City', 'Average_Sentiment_Score']\n",
    "\n",
    "# Step 10: Merge DataFrames and Calculate Overall Score\n",
    "city_texts = city_embeddings.merge(city_sentiment, on='City', how='left')\n",
    "city_texts['Average_Sentiment_Score'] = city_texts['Average_Sentiment_Score'].fillna(0)\n",
    "city_texts['Overall_Score'] = city_texts['Similarity'] * city_texts['Average_Sentiment_Score']\n",
    "\n",
    "# Step 11: Display the Recommendations\n",
    "city_texts = city_texts.sort_values(by='Overall_Score', ascending=False)\n",
    "print(\"\\nCity Recommendations Based on Your Preferences:\")\n",
    "recommendations = city_texts[['City', 'Similarity', 'Average_Sentiment_Score', 'Overall_Score']]\n",
    "print(recommendations.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using textblob instead of VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install sentence-transformers --quiet\n",
    "!pip install textblob --quiet\n",
    "!python -m textblob.download_corpora\n",
    "!pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 1: Load and Prepare the Data\n",
    "reddit_posts_file = 'reddit_posts_cities.csv'\n",
    "reddit_df = pd.read_csv(reddit_posts_file)\n",
    "\n",
    "# List the columns that contain messages\n",
    "message_columns = ['Title', 'Body', 'Comment 1', 'Comment 2', 'Comment 3', 'Comment 4', 'Comment 5']\n",
    "\n",
    "# Combine the messages into one column per row\n",
    "reddit_df['Combined_Messages'] = reddit_df[message_columns].fillna('').apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "# Ensure 'Subreddit' is treated as a string\n",
    "reddit_df['Subreddit'] = reddit_df['Subreddit'].astype(str)\n",
    "\n",
    "# Step 2: Define User Preferences (Attributes) with Synonyms\n",
    "attributes_of_interest = [\n",
    "    # 'clean', 'cleanliness', 'hygiene',\n",
    "    # 'rent', 'rental', 'housing', 'expensive', 'affordable',\n",
    "    # 'safe', 'safety', 'crime', 'dangerous', 'secure', 'unsafe'\n",
    "    'restaurants', 'transportation', 'cheap'\n",
    "]\n",
    "\n",
    "# Step 3: Load Sentence Transformer Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model suitable for semantic similarity\n",
    "\n",
    "# Step 4: Create Embeddings for Attributes\n",
    "attribute_embeddings = model.encode(attributes_of_interest, convert_to_numpy=True)\n",
    "mean_attribute_embedding = np.mean(attribute_embeddings, axis=0)\n",
    "\n",
    "# Step 5: Encode Individual Messages and Assign to DataFrame\n",
    "embeddings = model.encode(\n",
    "    reddit_df['Combined_Messages'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=32\n",
    ")\n",
    "reddit_df['Message_Embedding'] = list(embeddings)\n",
    "\n",
    "# Step 6: Compute Mean Embeddings for Each City\n",
    "city_embeddings = reddit_df.groupby('Subreddit')['Message_Embedding'].apply(\n",
    "    lambda embeddings: np.mean(np.vstack(embeddings), axis=0)\n",
    ").reset_index()\n",
    "city_embeddings.rename(columns={'Subreddit': 'City'}, inplace=True)\n",
    "\n",
    "# Step 7: Calculate Similarity Scores\n",
    "city_embeddings['Similarity'] = city_embeddings['Message_Embedding'].apply(\n",
    "    lambda emb: util.cos_sim(emb, mean_attribute_embedding).item()\n",
    ")\n",
    "\n",
    "# Step 8: Sentiment Analysis on Messages Mentioning Attributes Using TextBlob\n",
    "\n",
    "# Function to check if a message mentions any of the attributes\n",
    "def mentions_attributes(text, attributes):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return any(attr.lower() in tokens for attr in attributes)\n",
    "\n",
    "# Identify messages that mention the attributes\n",
    "reddit_df['Mentions_Attributes'] = reddit_df['Combined_Messages'].apply(\n",
    "    lambda text: mentions_attributes(text, attributes_of_interest)\n",
    ")\n",
    "\n",
    "# Filter messages that mention the attributes\n",
    "attribute_mentions_df = reddit_df[reddit_df['Mentions_Attributes']].copy()\n",
    "\n",
    "# Apply sentiment analysis to messages that mention attributes using TextBlob\n",
    "def get_sentiment_textblob(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Range from -1 (negative) to 1 (positive)\n",
    "\n",
    "attribute_mentions_df['Sentiment_Score'] = attribute_mentions_df['Combined_Messages'].apply(get_sentiment_textblob)\n",
    "\n",
    "# Step 9: Compute Average Sentiment Scores for Each City\n",
    "city_sentiment = attribute_mentions_df.groupby('Subreddit')['Sentiment_Score'].mean().reset_index()\n",
    "city_sentiment.columns = ['City', 'Average_Sentiment_Score']\n",
    "\n",
    "# Step 10: Merge DataFrames and Calculate Overall Score\n",
    "city_texts = city_embeddings.merge(city_sentiment, on='City', how='left')\n",
    "city_texts['Average_Sentiment_Score'] = city_texts['Average_Sentiment_Score'].fillna(0)\n",
    "city_texts['Overall_Score'] = city_texts['Similarity'] * city_texts['Average_Sentiment_Score']\n",
    "\n",
    "# Step 11: Display the Recommendations\n",
    "city_texts = city_texts.sort_values(by='Overall_Score', ascending=False)\n",
    "print(\"\\nCity Recommendations Based on Your Preferences:\")\n",
    "recommendations = city_texts[['City', 'Similarity', 'Average_Sentiment_Score', 'Overall_Score']]\n",
    "print(recommendations.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining city to city similarity - with handwavy sentiment adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "# ... (Assuming previous steps of data loading and processing have been done)\n",
    "\n",
    "# Extract city names and embeddings\n",
    "city_names = city_embeddings['City'].tolist()\n",
    "city_vectors = np.vstack(city_embeddings['Message_Embedding'].values)\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(city_vectors)\n",
    "\n",
    "# Adjust similarity matrix based on sentiment\n",
    "# Merge sentiment scores with city embeddings\n",
    "city_embeddings_sentiment = city_embeddings.merge(city_sentiment, on='City')\n",
    "\n",
    "# Extract sentiment scores\n",
    "sentiment_scores = city_embeddings_sentiment['Average_Sentiment_Score'].values\n",
    "\n",
    "# Use composite similarity (combining cosine similarity and sentiment similarity)\n",
    "# Calculate sentiment difference matrix\n",
    "sentiment_difference_matrix = np.abs(sentiment_scores[:, np.newaxis] - sentiment_scores[np.newaxis, :])\n",
    "\n",
    "# Normalize sentiment differences to range [0, 1]\n",
    "max_sentiment_diff = np.max(sentiment_difference_matrix)\n",
    "sentiment_similarity_matrix = 1 - (sentiment_difference_matrix / max_sentiment_diff)\n",
    "\n",
    "# Combine with cosine similarity matrix\n",
    "composite_similarity_matrix = similarity_matrix * sentiment_similarity_matrix\n",
    "\n",
    "# Create a DataFrame for the composite similarities\n",
    "composite_similarity_df = pd.DataFrame(composite_similarity_matrix, index=city_names, columns=city_names)\n",
    "\n",
    "# Convert composite similarity matrix to dissimilarity matrix\n",
    "dissimilarity_matrix = 1 - composite_similarity_matrix\n",
    "\n",
    "# Initialize MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "mds_results = mds.fit_transform(dissimilarity_matrix)\n",
    "\n",
    "# Create a DataFrame for MDS results\n",
    "mds_df = pd.DataFrame(mds_results, columns=['Dimension 1', 'Dimension 2'])\n",
    "mds_df['City'] = city_names\n",
    "\n",
    "# Plot the MDS results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(mds_df['Dimension 1'], mds_df['Dimension 2'])\n",
    "\n",
    "# Annotate points with city names\n",
    "for i, city in enumerate(mds_df['City']):\n",
    "    plt.annotate(city, (mds_df.loc[i, 'Dimension 1'], mds_df.loc[i, 'Dimension 2']))\n",
    "\n",
    "plt.title('MDS Plot of City Similarities (Adjusted for Sentiment)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining city to city similarity - without any sentiment adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ... (Assuming previous steps of data loading and processing have been done)\n",
    "\n",
    "# Step 5: Extract City Names and Embeddings\n",
    "city_names = city_embeddings['City'].tolist()\n",
    "city_vectors = np.vstack(city_embeddings['Message_Embedding'].values)\n",
    "\n",
    "# Step 6: Calculate the Cosine Similarity Matrix Between Cities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "similarity_matrix = cosine_similarity(city_vectors)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=city_names, columns=city_names)\n",
    "\n",
    "# Display the similarity matrix\n",
    "print(\"Pairwise Cosine Similarities Between Cities:\")\n",
    "print(similarity_df)\n",
    "\n",
    "# Step 7: Visualize Similarities Using MDS (Without Sentiment Analysis)\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Convert similarity matrix to dissimilarity matrix (distance matrix)\n",
    "# Since cosine similarity ranges from -1 to 1, and MDS requires distances, we can use 1 - cosine similarity\n",
    "dissimilarity_matrix = 1 - similarity_df\n",
    "\n",
    "# Initialize MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "mds_results = mds.fit_transform(dissimilarity_matrix)\n",
    "\n",
    "# Create a DataFrame for MDS results\n",
    "mds_df = pd.DataFrame(mds_results, columns=['Dimension 1', 'Dimension 2'])\n",
    "mds_df['City'] = city_names\n",
    "\n",
    "# Plot the MDS results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(mds_df['Dimension 1'], mds_df['Dimension 2'])\n",
    "\n",
    "# Annotate points with city names\n",
    "for i, city in enumerate(mds_df['City']):\n",
    "    plt.annotate(city, (mds_df.loc[i, 'Dimension 1'], mds_df.loc[i, 'Dimension 2']))\n",
    "\n",
    "plt.title('MDS Plot of City Similarities (Without Sentiment Analysis)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
